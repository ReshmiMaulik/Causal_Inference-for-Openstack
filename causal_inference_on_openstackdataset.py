# -*- coding: utf-8 -*-
"""Causal inference on Openstackdataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRlz2rxwTbJUPaeglMHdPL8aqLq27CgB
"""

!pip install git+https://github.com/microsoft/dowhy.git
import dowhy
from dowhy import CausalModel

import numpy as np
import pandas as pd
import graphviz
import networkx as nx

np.set_printoptions(precision=3, suppress=True)
np.random.seed(0)

# Import data from Github

url="https://raw.githubusercontent.com/ReshmiMaulik/Understanding-Developer-Interaction/main/Data/workingdataO.csv"


#df = pd.read_csv(url, sep="\t", header= None)
df = pd.read_csv(url, sep=",")

df.info()

# Display the summary statistics in a pretty way
summary_stats = df.describe()
styled_summary_stats = summary_stats.style.format("{:.2f}").set_caption("Summary Statistics")
display(styled_summary_stats)

df

import matplotlib.pyplot as plt

import seaborn as sns # Import seaborn library
corr = df.corr() # Calculates correlation matrix
# Create the heatmap
plt.figure(figsize=(10, 8))
ax = sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
#sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, annot_kws={"size": number_cex})
#plt.title("Correlation Matrix")
plt.show()

#--new graph----Final code
causal_graph = """strict digraph  {

OW->RI;
RI->ET;
OW->ET;
OE->ET;

}

"""

!apt install libgraphviz-dev
!pip install pygraphviz

from dowhy import CausalModel
from IPython.display import Image, display

model= CausalModel(
        data = df,
        graph=causal_graph.replace("\n", " "),
        treatment='RI',       #ndev
        outcome='ET')

#model.view_model()
model.view_model(file_name="causal_model.png")

# For Mediation analysis..Not required
from dowhy import CausalModel
from IPython.display import Image, display

model= CausalModel(
        data = df,
        graph=causal_graph.replace("\n", " "),
        treatment='RI',       #ndev
        outcome='ET',
        missing_nodes_as_confounders=True)

#model.view_model()
model.view_model(file_name="causal_model.png")

"""Identify the causal effect:"""

estimands = model.identify_effect()
print(estimands)

"""Estimate"""

estimate= model.estimate_effect(
 identified_estimand=estimands,
 method_name='backdoor.linear_regression',
 confidence_intervals=True,
  test_significance=True
)

print(f'Estimate of causal effect: {estimate}')

"""Conditional Estimates
__categorical__OE
(-0.001, 39.0]       9.636368
(39.0, 139.0]        9.592247
(139.0, 399.0]       9.478583
(399.0, 1396.0]      9.148191
(1396.0, 57002.0]    7.233890

Refutation

Refuting the estimate
"""

refutel_common_cause=model.refute_estimate(estimands,estimate,"random_common_cause")
print(refutel_common_cause)

"""Refute: Add a random common cause
Estimated effect:9.025651559477865
New effect:9.025893347968985
p value:0.98

Final:  Refute: Add a random common cause
Estimated effect:9.025651559477865
New effect:9.02458758720755
p value:0.86

Removing a random subset of the data
"""

refutel_common_cause=model.refute_estimate(estimands,estimate,"data_subset_refuter")
print(refutel_common_cause)

"""Refute: Use a subset of data
Estimated effect:9.025651559477865
New effect:8.90466268310124
p value:0.84

Final->Refute: Use a subset of data
Estimated effect:9.025651559477865
New effect:9.065842742424303
p value:1.0

Replacing treatment with a random (placebo) variable
"""

refutation = model.refute_estimate(estimands, estimate, method_name="placebo_treatment_refuter", placebo_type="permute", num_simulations=100)
print(refutation)

"""Refute: Use a Placebo Treatment
Estimated effect:9.025651559477865
New effect:-0.08778022345330783
p value:0.84

Final-> Placebo treatment Estimated effect:9.025651559477865
New effect:-0.04398494981694512
p value:0.92
"""

es_random=model.refute_estimate(estimands,estimate, method_name="random_common_cause", show_progress_bar= True)

print(es_random)

#Add an Unobserved Common Cause
res_unobserved=model.refute_estimate(estimands, estimate, method_name="add_unobserved_common_cause",
                                     confounders_effect_on_treatment="binary_flip", confounders_effect_on_outcome="linear",
                                    effect_strength_on_treatment=0.01, effect_strength_on_outcome=0.02)
print(res_unobserved)

"""**Step 2: Identifying the natural direct and indirect effects**

We use the estimand_type argument to specify that the target estimand should be for a natural direct effect or the natural indirect effect. For definitions, see Interpretation and Identification of Causal Mediation by Judea Pearl.

Natural direct effect: Effect due to the path OW->ET

Natural indirect effect: Effect due to the path OW->RI->ET (mediated by RI).
"""

# Natural direct effect (nde)
identified_estimand_nde = model.identify_effect(estimand_type="nonparametric-nde",
                                            proceed_when_unidentifiable=True)
print(identified_estimand_nde)

# Natural indirect effect (nie)
identified_estimand_nie = model.identify_effect(estimand_type="nonparametric-nie",
                                            proceed_when_unidentifiable=True)
print(identified_estimand_nie)

"""Step 3: Estimation of the effect
Natural Indirect Effect **bold text**

Natural Direct Effect
"""

import pandas as pd

url="https://raw.githubusercontent.com/ReshmiMaulik/Understanding-Developer-Interaction/main/Data/workingdataO.csv"

df = pd.read_csv(url,header=None)

#df = fp.load_data(url)  # companion example data
df.head(5)

import pandas as pd
from sklearn.preprocessing import StandardScaler

url = "https://raw.githubusercontent.com/ReshmiMaulik/Understanding-Developer-Interaction/main/Data/workingdataO.csv"
df = pd.read_csv(url, header=None)

# Convert all columns to numeric, errors='coerce' will replace non-numeric values with NaN
df = df.apply(pd.to_numeric, errors='coerce')

# Select only numerical columns for scaling (ignoring columns with all NaN values)
numerical_cols = df.select_dtypes(include=['number']).columns[df.select_dtypes(include=['number']).any()]  # Added any() condition
df_numerical = df[numerical_cols]

# Apply StandardScaler to numerical columns only
ss = StandardScaler()
df_scaled_numerical = ss.fit_transform(df_numerical)

# Create a new DataFrame with scaled numerical features
df_scaled = pd.DataFrame(df_scaled_numerical, columns=numerical_cols, index=df.index)

# Concatenate scaled numerical features with original non-numerical features
non_numerical_cols = df.select_dtypes(exclude=['number']).columns
df_scaled = pd.concat([df_scaled, df[non_numerical_cols]], axis=1)

# Print the scaled DataFrame
print(df_scaled.head())

"""SEM in Python"""

!pip install semopy

import semopy as sem
import pandas as pd
import numpy as np

#Mediation analysis

specmodel= """
# measurement model
# Direct Effect
ET ~ c * OW
RI ~ a * OW
ET ~ b * RI
"""
# Label the effects
#direct := c
# indirect effect
#indirect := a* b
# total := direct + indirect
#prop_mediated := indirect/total

import semopy
# Define the model
model = semopy.Model(specmodel)
#Fit the model

model.fit(df)
# Inspect the results
print(model.inspect())

!brew install graphviz

import matplotlib.pyplot as plt # Import the matplotlib.pyplot module

semopy.semplot(model, 'openstack_sem_model.png')
print("SEM Model diagram saved as 'openstack_sem_model.png'.")
img = plt.imread('openstack_sem_model.png')
plt.imshow(img)
plt.axis('off')
plt.show()

model.fit(df, obj="MLW", solver="SLSQP")

"""we observe the standard fitting value"""

model.inspect(mode='list', what="names", std_est=True)

"""We calculate our model adjustment indicators"""

sem.calc_stats(model)

g = sem.semplot(model, "model.png")

from semopy import Model

# Assuming 'specmodel' is your semopy Model instance and it's been fitted
std_estimates = model.inspect(mode='list', what="names", std_est=True) # Call inspect as a method of the Model object
std_estimates = std_estimates[std_estimates["op"] == "~"] # display regression coefficients only

std_estimates

m = semopy.Model(specmodel)
m.fit(df)

m.inspect()

"""To calculate Cronbach's Î± coefficient in Python, you can use the pingouin or scipy library."""

!pip install pingouin

import pandas as pd
import pingouin as pg

# Assuming your dataframe is named 'data'
cronbach_alpha = pg.cronbach_alpha(data)
print(cronbach_alpha)

